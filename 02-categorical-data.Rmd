# Working with categorical data

## Introduction

Creating and manipulating categorical data sets requires some skills and techniques in R beyond those ordinarily used for quantitative data. This chapter illustrates these for the main formats for categorical data: case form, frequency form and table form.

> The chapter uses many base R commands I am not familiar. But I believe that most of these command could be translated to the tidy data approach. The praxis will show, if I have the skills for these transformations. In this chapter I will focus on those command where I lack knowledge of their function either in base R or tidy approach.

> For example there is a focus on matrices and arrays, data structures I am not comfortable with. But it should be easy to convert them to data frames or tibbles. Similiar with the graphic command `plot()` which I will replace with `ggplot().`

## Forms of categorical data

> The book distinguishes between case, frequency and table form. I believe that this differentiation is with the tidy approach not essential anymore. More important nowadays is the conversion of data from wide to long with `pivot_longer()` and vice versa with `pivot_wider()`. But again: Practical challenges will be the proof of my assessment.


### Using tabulizer

Instead of entering the data manually or importing from a file or website, I was experimenting with the **{tabulizer}** package. 'Tabula' is a Java library designed to computationally extract tables from PDF documents. 

Sometimes there are problems with the employment as **{rJava}** is required and complicated to install. But I had it already installed and cant remember any problems. **{tabulizerjars}** is also necessary. The only purpose of **{tabulizerjars}** is to distribute releases of the 'Tabula' Java library to be used with the **{tabulizer}** package. Note that the package itself does not provide any functionality apart from basic linking to Java version installed on the system.

See the [Tabula](https://tabula.technology/) webpage but first and foremost the [rOpenSci web page on tabulizer](https://docs.ropensci.org/tabulizer/index.html) for more details and documentation of all the features.

The main function is `extract_tables()` with several parameters, for instance the page for the table to extracted. But in my case to provide the page resulted in the extraction of another table at the beginning of the page. Therefore I had to use the interactive `locate_areas()` function. The program stops and displays a mini picture of the desired PDF page where one can select the area with the table.


```{r locate-areas, eval=FALSE}
library(tidyverse)
library(tabulizer)
library(tabulizerjars)

f <- normalizePath("test-data/DDAR.pdf")
tab_area <- locate_areas(file = f, page = 53)
saveRDS(tab_area, "test-data/example-tab-area")
```

```{r extract-area}
library(tidyverse)
library(tabulizer)
library(tabulizerjars)

f <- normalizePath("test-data/DDAR.pdf")
tab_area <- readRDS("test-data/example-tab-area")
df <- extract_tables(file = f,
                    page = 53, 
                    area = tab_area,
                    guess = FALSE,
                    output = "data.frame")
df <- tibble(df[[1]])
colnames(df) <- as.character(df[1, ])
df <- df[-1,]
df <- pivot_longer(df, 2:4, names_to = "party", values_to = "count")
df
```


### Generating tables

With data in case form or frequency form, you can generate frequency tables from factor variables in data frames using the `table()` function; for tables of proportions, use the `prop.table()` function, and for marginal frequencies (summing over some variables) use `margin.table()`.

::: {.todobox}
> I have to look up other books using modern approaches to see what functions they use, e.g., [ModernDive](https://moderndive.netlify.app/index.html) (Statistical Inference via Data Science -- A ModernDive into R and the Tidyverse). I believe the **{janitor}** packages may be helpful for `prop.table()` and `margin.table()` functionalities.
:::

### Printing tables

For 3-way and larger tables, the functions `ftable()` (in the **{stats}** package) and `structable()` (in **{vcd}**) provide a convenient and flexible tabular display in a “flat” (2-way) format.

### Collapsing tables

It sometimes happens that we have a data set with more variables or factors than we want to analyze. The book recommends `aggregate()` in {stats}, `margin.table()` resp. `marginSums()`, `apply()` both in {base}, and collapse.table()` from the **{vcdExtra}** package.

::: {.todobox}
> Again I have to find equivalents. I believe that the modern approach works with **{dplyr}**, **{tidyr}**, **{purrr}**, **{tibble}** and **{forcats}** (all provided by the **{[tidyverse](https://tidyverse.tidyverse.org/)}** package suite.)
:::

### Converting tables

A given contingency table can be represented equivalently in case form, frequency form, and table form. However, some R functions were designed for one particular representation. Therefore converting tables from one form to another, are critical.

In addition to the already mentioned functions of `table()`, `xtabs()` and `as.data.frame()` the book references the `expand.dft()` function in **{vcdExtra}**. For producing a LateX table the **{xtable}** package is recommended.

> Am not sure if the tidyverse() packages solves these conversion problems. But to know more about the requirements I will print the output from different table commands.


I am going to use for this task the `UCBAdmissions` data from the base **{datasets}** package. For shorter reference I will copy it to `UCB.`

**UCBAdmissions**:The original structure of the table:

```{r print-standard-table}
(UCB <- UCBAdmissions)
```


**ftable()**:

```{r print-ftable}
ftable(UCB)
```

**ftable with formula method**: 

```{r print-ftable-formula}
# ftable(UCB, row.vars = 1:2)      # same result
ftable(Admit + Gender ~ Dept, data = UCB)   # formula method
```

**as_tibble()**: 

```{r print-tibble}
as_tibble(UCB)
```

**structable()**:

```{r print-structable}
library(vcd)
structable(UCB)
```

> It seems to me that all the complex discussion about different table formats is now outdated with **{tibble}** from the **{tidyverse}** approach. The same is true with **{dplyr}** concerning subsetting, filtering or other transforming activities. This is especially important for manipulating factor levels where the **{forcats}** packages replaces all the different function for aggregating and collapsing. But to learn & decide what code is necessary I would need the actual problem. Therefore I just skimmed chapter 2.

## A complex example

> A good conversion exercise is the following complex example on TV viewing data. So I can check if I am able to provide the necessary R code to fulfil the requirements. I have to inspect it line per line and relace it with working code. To see the difference I will keep the old code as comments. Also I wll describe my transforming action.


### Dataset description

Are you ready for a more complicated example that puts together a variety of the skills developed in this chapter? These skills are 

(a) reading raw data, 
(b) creating tables, 
(c) assigning level names to factors and 
(d) collapsing levels or variables for use in analysis. 

For an illustration of these steps, we use the dataset `tv.dat`, supplied with the initial im- plementation of mosaic displays in R by Jay Emerson. In turn, they were derived from an early, compelling example of mosaic displays that illustrated the method with data on a large sample of TV viewers whose behavior had been recorded for the Neilsen ratings. This data set contains sample television audience data from Neilsen Media Research for the week starting November 6, 1995. 

The data file, `tv.dat`, is stored in frequency form as a file with 825 rows and 5 columns. There is no header line in the file, so when we use `read.table()` below, the variables will be named `V1 – V5`. This data represents a 4-way table of size $5 × 11 × 5 × 3 = 825$ where the table variables are `V1 – V4`, and the cell frequency is read as `V5`. 

The table variables are: 

- `V1` --- values $1:5$ correspond to the days Monday–Friday; 
- `V2` --- values $1:11$ correspond to the quarter-hour times $8:00$ pm through $10:30$ pm; 
- `V3` --- values $1:5$ correspond to ABC, CBS, NBC, Fox, and non-network choices; 
- `V4` --- values $1:3$ correspond to transition states: turn the television Off, Switch channels, or Persist in viewing the current channel.

### Creating data frames and arrays 

#### Package dataset {#package-dataset}

There is a `TV` dataset in the **{vcdExtra}** package. To load it would be the easiest way to get the data. In that case you would not have to worry about data transformation because the dataset is already in the desired form.

```{r dataset-import, collapse=TRUE}
library(vcdExtra)

data(TV)  # the easiest way, does not need data wrangling
TV
str(TV)
```
The data set `TV` comprises a 5 x 11 x 3 contingency table. But this is not the original dataset described under the subsection \@ref(package-dataset).

"The original data, tv.dat, contains two additional networks: "Fox" and "Other", with small frequencies. These levels were removed in the current version. There is also a fourth factor, transition State transition (turn the television Off, Switch channels, or Persist in viewing the current channel). The TV data here includes only the Persist observations." (From the TV **{vcdExtra}** help file.)


We therefore will go the hard way and import the `tv.dat` file as mentioned in the book. But viewing the above dataset gives you a good impression how the data should look at the end of the data wrangling process.



::: {.bluebox}
The **b**ook **v**ersion contains` bv-` in the chunk- and variable names. In contrast to my own version, which has my initials `pb-` in their designations.
:::


#### bv-import

The provided R code in the [ch02.R file](http://ddar.datavis.ca/pages/Rcode/ch02.R) does not work. The book referenced the file `tv.dat` to the `doc/extdata` directory of **{vcdExtra}**. On my (macOS) installation the data is found also inside `extdata`, but `extdata` is on the highest level and therefore not a subdirectory of `doc`.

You can also use the RStudio interactive menu: "File -> Import Dataset -> From text (base) …".

In the next step we use `xtabs()` to do the cross-tabulation, using $V5$ as the frequency variable. `xtabs()` uses a formula interface.

I HAVE XTABS TO EXPLAIN ABOVE UND ALSO NEED TO DISPLAY AN EXAMPLE. (Chapter 2.4.2, p.46 / 66)

The third step attach names to the factors. There is no assignment necessary but the list has to be ordered.

In summary the "old" approach is shorter and seems less complicated as the "new" tidy approach. EXPLANATION?? Look if there is with tv_pb the possibility to name the columns and levels immediately when importing the data file!


```{r tv-bv, collapse=TRUE}
tv_bv <-
    read.table(system.file("extdata", "tv.dat",   # without "doc" directory
                           package = "vcdExtra"))

tv_bv <- xtabs(V5 ~ ., data = tv_bv)

dimnames(tv_bv) <-
    list(
        Day = c("Mon", "Tue", "Wed", "Thu", "Fri"), 
        Time = c(
            "8:00",
            "8:15",
            "8:30",
            "8:45",
            "9:00",
            "9:15",
            "9:30",
            "9:45",
            "10:00",
            "10:15",
            "10:30"
        ),
        Network = c("ABC", "CBS", "NBC", "Fox", "Other"),
        State = c("Off", "Switch", "Persist")
    )

tv_bv <- as.data.frame(tv_bv, dim = c(5, 11, 5, 3))

str(tv_bv)
head(tv_bv, 5)
```




#### pb-import


I will use the `read_table()` function from the **{readr}** package to import the data into a [tibble](https://tibble.tidyverse.org/). 

::: {.warningbox}
Watch the difference between `bv` and `pb` version: The function `read.table()` (with a period) in `bv` is called from the **{base}** package, whereas `read_table()` (with an underscore) in `pb` is part of **{readr}** and has to be loaded as a library.
:::

```{r pb-import, collapse=TRUE}
library(tidyverse) # contains {readr} and {tibble}

tv_pb <-
    read_fwf(system.file("extdata", "tv.dat",   # without the "doc" directory
                         package = "vcdExtra"),
        col_types = 'ffffi',
        fwf_widths(c(2, 2, 2, 2, 2)
    )) |>
    rename(c(
        Day = X1,
        Time = X2,
        Network = X3,
        State = X4,
        Freq = X5
    )) |>
    mutate(Day = fct_recode(
        Day,
        Mon = "1",
        Tue = "2",
        Wed = "3",
        Thu = "4",
        Fri = "5"
    )) |>
    mutate(
        Time = fct_recode(
            Time,
            "8:00" = "1",
            "8:15" = "2",
            "8:30" = "3",
            "8:45" = "4",
            "9:00" = "5",
            "9:15" = "6",
            "9:30" = "7",
            "9:45" = "8",
            "10:00" = "9",
            "10:15" = "10",
            "10:30" = "11"
        )
    ) |>
    mutate(Network = fct_recode(
        Network,
        "ABC" = "1",
        "CBS" = "2",
        "NBC" = "3",
        "Fox" = "4",
        "Other" = "5"
    )) |>
    mutate(State = fct_recode(
        State,
        "Off" = "1",
        "Switch" = "2",
        "Persist" = "3"
    ))

str(tv_pb)
glimpse(tv_pb)
head(tv_pb, 5)
```

Besides the variables' different naming schemes, we get a separate sixth column of a logical type where all values are `NA` (not available). I do not know where this extra empty column comes from.


### Renaming variables (columns)

We need to rename the variables, and converting the integer-coded factors V1 – V4 to R factors. 

#### bv

The lines below use the function `within()` to avoid having to use `TV.dat$Day <- factor(TV.dat$Day)` etc., and only supply labels for the first variable.

```{r bv-rename-vars}
TV_df <- tv_data_bv
colnames(TV_df) <- c("Day", "Time", "Network", "State", "Freq")
TV_df <- within(TV_df, {
           Day <- factor(Day, 
                         labels = c("Mon", "Tue", "Wed", "Thu", "Fri"))
           Time <- factor(Time)
           Network <- factor(Network)
           State <- factor(State) 
	 })
```


#### pb

What happened with 6th column? Why is data of logical type?
Rename variable and set correct data type during import!


```{r final-example, eval=FALSE}
### Check with the original example, TV viewing data, p. 58ff.
library(vcdExtra)
## reading in the data
tv_data <- TV
str(tv_data)
head(tv_data, 5)

## tv_data <- read.table("C:/R/data/tv.dat")

## tv_data <- read.table(file.choose())

## creating factors within the data frame
TV_df <- tv_data
colnames(TV_df) <- c("Day", "Time", "Network", "State", "Freq")
TV_df <- within(TV_df, {
           Day <- factor(Day,
                         labels = c("Mon", "Tue", "Wed", "Thu", "Fri"))
           Time <- factor(Time)
           Network <- factor(Network)
           State <- factor(State)
	 })

## reshaping the table into a 4-way table
TV <- array(tv_data[,5], dim = c(5, 11, 5, 3))
dimnames(TV) <-
    list(c("Mon", "Tue", "Wed", "Thu", "Fri"),
         c("8:00", "8:15", "8:30", "8:45", "9:00", "9:15",
           "9:30", "9:45", "10:00", "10:15", "10:30"),
         c("ABC", "CBS", "NBC", "Fox", "Other"),
         c("Off", "Switch", "Persist"))
names(dimnames(TV)) <- c("Day", "Time", "Network", "State")

## Creating the table using xtabs()
TV <- xtabs(V5 ~ ., data = tv_data)
dimnames(TV) <-
    list(Day = c("Mon", "Tue", "Wed", "Thu", "Fri"),
         Time = c("8:00", "8:15", "8:30", "8:45", "9:00", "9:15",
                  "9:30", "9:45", "10:00", "10:15", "10:30"),
         Network = c("ABC", "CBS", "NBC", "Fox", "Other"),
         State = c("Off", "Switch", "Persist"))

### SECTION ### 2.9.2. Subsetting and collapsing

## subsetting data
TV <- TV[,,1:3,]     # keep only ABC, CBS, NBC
TV <- TV[,,,3]       # keep only Persist -- now a 3 way table
structable(TV)

## collapsing time labels
TV2 <- collapse.table(TV,
                      Time = c(rep("8:00-8:59", 4),
                               rep("9:00-9:59", 4),
			       rep("10:00-10:44", 3)))
structable(Day ~ Time + Network, TV2)

```

