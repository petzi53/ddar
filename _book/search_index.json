[["index.html", "Discrete Data Analysis with R Preface", " Discrete Data Analysis with R Peter Baumgartner 2022-02-10 Preface These are my excerpts and personal notes on the “Discrete Data Analysis with R” textbook by Michael Friendly and David Meyer. Friendly, M., &amp; Meyer, D. (2015). Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data (1st ed.). Chapman and Hall/CRC. My notes on (Friendly and Meyer 2015) are part excerpt and part personal notes (comments). Most of the text are quotes taken directly from the book, which I have not marked especially. Sometimes I commented or put forward my own thought. In those cases I have formatted the text as a quotation. This is an example of a paragraph marked as a quotation. These type of formatted text passages present — contrary to the usual formatting habits — my own thoughts, whereas excerpts or quotes from the textbook is not marked specially. R scripts and data files are taken from the book website. The book is supported directly by the R packages {vcd} and {vcdExtra}, along with numerous other R packages. There is a list of packages used a list of the data sets used in the book, by package and a list of files with R code per chapter or as a zip file. References "],["introduction.html", "1 Introduction 1.1 Overview 1.2 What is categorical data? 1.3 Data analysis strategies 1.4 Graphical methods 1.5 The 80-20-rule 1.6 Visualization of distributions", " 1 Introduction Friendly, M., &amp; Meyer, D. (2015). Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data (1st ed.). Chapman and Hall/CRC. Short name (abbreviation): DDAR 1.1 Overview “Discrete Data Analysis with R” (Friendly and Meyer 2015) has at its strength the interplay between analysis and visualization strategies for categorical data. The authors drive the conviction that data analysis and statistical graphics should go hand-in-hand in the process of understanding and communicating statistical data. The book features the well-known {vcd} package (Visualizing Categorical Data), and the books support package {vcdExtra} (‘vcd’ Extensions and Additions), which provides additional data sets and methods. Both packages come with large tutorial vignettes explaining the usage. See for instance: Working with categorical data with R and the vcd and vcdExtra packages: 37 pages The Strucplot Framework: Visualizing Multi-way Contingency Tables with vcd: 48 pages The book divides statistical methods for data analysis into two different categories: Randomized-based methods: Pearson chi-squared \\(X^2\\) test, Fisher’s exact test, and Cochran–Mantel–Haenszel tests (Chapter 3-6) and Model-based methods: Logistic regression, loglinear, and generalized linear models (Chapter 7-9). 1.2 What is categorical data? Categorical variables differ in the number of categories: binary variables (or dichotomous variables) such as Gender: “Male,” “Female” multilevel variables, variables with more than two categories (called polytomous variables), e.g., Fielding position (in baseball), with categories “Pitcher,” “Catcher,” “1st base,” “2nd base,” …, “Left field.” their relation: ordered categories: e.g., Treatment outcome: “No improvement,” “Some improvement,” “Marked improvement.” nominal categories: e.g., Marital Status: “Never married,” “Married,” “Separated,” “Divorced,” “Widowed.” Often, we can rank some variable under a specific point of view. For instance, we could arrange the variable side effect in a pharmacological study according to the seriousness of the side effects or party preference according to the number of party voters in the last election. But these kinds of classifications are somewhat arbitrary. An ordinal variable should be defined as one whose categories are unambiguously ordered along a single underlying dimension. Examples are levels of education, for instance the nine levels of the International Standard Classification of Education (ISCED): early childhood education (level 0); primary education (level 1); lower secondary education (level 2); upper secondary education (level 3); postsecondary non-tertiary education (level 4); short-cycle tertiary education (level 5); bachelor’s or equivalent level (level 6); master’s or equivalent level (level 7); doctor or equivalent level (level 8) or the Likert scale in survey questions (“strongly agree,” “agree,” “neutral,” “disagree,” “strongly disagree.”). 1.2.1 Case vs. frequency form In many circumstances, data is recorded on each individual or experimental unit. Data in this form is called case data, or data in case form. Data in frequency form, by contrast, has already been tabulated by counting over the categories of the table variables. If observations are recorded from operationally independent experimental units or individuals, typically a sample from some population, this tabulated data may be called frequency data. However, suppose several events or variables are observed for the same units or individuals. Those events are not operationally independent, and it is helpful to use the term count data in this situation. These terms are by no means standard, but the distinction is often essential, particularly in statistical models for categorical data. For example, in a tabulation of the number of male children within families, the number of male children in a given family would be a count variable, taking values 0, 1, 2, …. The number of independent families with a given number of male children is a frequency variable. Count data also arise when we tabulate a sequence of events over time or under different circumstances in a number of individuals. 1.2.2 Univariate, bivariate, and multivariate data Another distinction concerns the number of variables: one, two, or (potentially) many shown in a data set or table or used in some analysis. Most statistical models distinguish between response variables (or dependent or criterion variables) and explanatory variables (or independent or predictor variables). 1.2.3 Explanatory vs. response variables Response Variable (also dependent or criterion variable) is either the treatment outcome (primarily quantitative), e.g., mean -&gt; normal distribution or variable of primary interest (e.g., educational achievement level predicted from socioeconomic status) In the classical models, the response variable (“treatment outcome,” for example) must be considered quantitative. The model attempts to describe how the mean of the distribution of responses changes with the values or levels of the explanatory variables, such as age or gender. However, when the response variable is categorical, the standard linear models do not apply because they assume a normal (Gaussian) distribution for the model residuals. Explanatory Variable (independent or predictor variable) experimentally manipulated, e.g., the treatment group each person is assigned to or not manipulated (e.g., socioeconomic status), but used to predict another variable Hence, a categorical response variable generally requires analysis using methods for categorical data, but categorical explanatory variables may be readily handled by either method. 1.3 Data analysis strategies 1.3.1 Hypothesis testing approaches Many studies raise questions concerning hypotheses about the association between variables, a more general idea than that of correlation (linear association) for quantitative variables. If a non-zero association exists, we may wish to characterize the strength of the association numerically and understand the pattern or nature of the association. Examples for Questions: “Is there evidence of gender bias in admission to graduate school?” Another way to frame this: “Are males more likely to be admitted?” Questions involving tests of such hypotheses are answered most easily using a large variety of specific statistical tests, often based on randomization arguments. These include the familiar Pearson chi-squared test for two-way tables, the Cochran–Mantel–Haenszel test statistics, Fisher’s exact test, and a wide range of measures of strength of association. The hypothesis testing approach is illustrated in Chapters 4–6 of DDAR, though the emphasis is on graphical methods that help us understand the nature of the association between variables. Illustration of hypothesis testing The approach is demonstrated with the HairEyeColor from the base R {datasets} package. As a graphical tool, it uses mosaic display mosaicplot() from the base R {graphics} package. (see Chapter 5 of DDAR) and correspondence analysis plot ca() from the {ca} package (see Chapter 6 of DDAR) For the test calculation, it uses chisq.test() from the base R {stats} package and assocstats() from the {vcd} package. I do not know how to apply these tools. I will postpone my reading until chapters 5 and 6. 1.3.2 Model building approaches Model-based methods provide tests of equivalent hypotheses about associations but offer additional advantages (at the cost of additional assumptions) not provided by the simpler hypotheses-testing approaches. Among these advantages, model-based methods provide estimates, standard errors, and confidence intervals for parameters and obtain predicted (fitted/expected) values with associated precision measures. Illustration of model building The book presents for demonstration logistic regression of dichotomous response variables. It illustrates the approach with two important real-world examples: Space shuttle disaster: dataset SpaceShuttle from {vcd}. Survivals of the Donner party (1846/47 Sierra Nevada): dataset Donner from {vcdExtra}. Again I do not understand fully how the model building is done. I am looking forward to reading chapter 7 of DDAR. 1.4 Graphical methods 1.4.1 Visualization recommendations This book section presents several advantages of data visualization. Note that data analysis and graphing are iterative processes: Visualization = Graphing + Fitting + Graphing + Fitting + Graphing … Presentation vs. Exploration: Different communication purposes require different graphs. There is, for instance, a big difference between graphs for presentation and for data exploring. The book’s text links to Statistical Graphs and More a blog on visualization by Martin Theus. There are – for instance – on the blog side column several links to interesting books by the blog author and other scientists I will mention here (but not include them in the bibliography of this book) for my further involvement with the visualization subject: Some books on visualization topics - Cook, D., &amp; Swayne, D. F. (2007). Interactive and Dynamic Graphics for Data Analysis. - Theus, M., &amp; Urbanek, S. (2019). Interactive graphics for data analysis: Principles and examples. - Unwin, A., Theus, M., &amp; Hofmann, H. (2016). Graphics of Large Datasets: Visualizing a million. Springer-Verlag New York. Different graphical methods: Frequencies of categorical variables are often best represented graphically using areas rather than as positions along a scale. Three examples are just illustrated with pictures. There is no R code because the images only serve as a reference for more detailed explanations in later chapters. I will link these three examples to more detailed web pages: mosaic plot (a modified bar chart), fourfold display, and agreement chart. Details on these visualization methods are provided in later chapters. Effect ordering: Alphabetically sorted labels for ordered categories result in most of the time in nonsensical display. Therefore sort the data by the effects to be seen to facilitate comparison. This so-called “effect-order sorting” in combination with a colored representation is also possible with the data fields of the table itself. Interactive and dynamic graphs: Graphics displayed in print form are necessarily static and fixed when designed and rendered as an image. Yet, recent developments in software, web technology, and media alternative to print have created the possibility to extend graphics in far more helpful and exciting ways for both presentation and analysis purposes. Interactive graphics allow the viewer to directly manipulate the statistical and visual components of the graphical display. These range from graphical controls (sliders, selection boxes, and other widgets) to control details of analysis (e.g., a smoothing parameter) or graph (colors and other graphic details), to higher-level interaction, including zooming in or out, drilling down to a data subset, linking multiple displays, selecting terms in a model, and so forth. The substantial effect is that the analysis and/or display is immediately re-computed and updated visually. In addition, dynamic graphics use animation to show a series of views, like frames in a movie. Adding time as an additional dimension allows far more possibilities, for example showing a rotating view of a 3D graph or showing smooth transitions or interpolations from one view to another. There are now many packages in R providing interactive and dynamic plots. I will only mention them here, planning to look into the details of these packages. Note that the references are from 2015, meaning that there are now even more packages providing interactive graphics and animated plots. On the other hand, some of the mentioned packages may be outdated. 1.4.2 Recommended Packages Packages for interactive and dynamic plots rggobi: A command-line interface to ‘GGobi,’ an interactive and dynamic graphics package. ‘Rggobi’ complements the graphical user interface of ‘GGobi,’ providing a way to fluidly transition between analysis and exploration, as well as automating common tasks. But the package was removed from the CRAN repository. Older version are in the archive. – GGobi is not an R package but an open-source visualization program for exploring high-dimensional data. It provides highly dynamic and interactive graphics such as tours and familiar graphics like the scatterplot, bar chart, and parallel coordinates plots. Plots are interactive and linked with brushing and identification. GGobi is fully documented in the GGobi book Interactive and Dynamic Graphics for Data Analysis. There are many references on using GGobi with R, including code examples. iPlots: interactive graphics for R, see the website for detailed information and tutorials. (Last change: June 17, 2018.) rCharts: rCharts is an R package to create, customize and publish interactive javascript visualizations from R using a familiar lattice style plotting interface. See also the not related website R Charts with the especially interesting ggplot2 section. ggogleVis: R interface to Google’s chart tools, allowing users to create interactive charts based on data frames. Charts are displayed locally via the R HTTP help server. A modern browser with an Internet connection is required. The data remains local and is not uploaded to Google. Several vignettes demonstrate the application. ggvis: Interactive Grammar of Graphics. An implementation of an interactive grammar of graphics, taking the best parts of ‘ggplot2,’ combining them with the reactive framework of ‘shiny,’ and drawing web graphics using ‘vega.’ See website at https://ggvis.rstudio.com/. animation: A gallery of animations in statistics and utilities to create animations. See website https://yihui.org/animation/. manipulate: Interactive Plots for RStudio. Interactive plotting functions for use within RStudio. The manipulate function accepts a plotting expression and a set of controls (e.g. slider, picker, checkbox, or button) used to dynamically change values within the expression. The expression is automatically re-executed when a value is changed using its corresponding control, and the plot is redrawn. During searching the link of the above packages I came across two other visualization packages. I will list the for later detailed consulting: apexcharter: Create Interactive Chart with the JavaScript ‘ApexCharts’ Library. Provides an ‘htmlwidgets’ interface to ‘apexcharts.js.’ ‘Apexcharts’ is a modern JavaScript charting library to build interactive charts and visualizations with simple API. See ‘Apexcharts’ examples and documentation at https://apexcharts.com/. chronochrt: Creating Chronological Charts with R. Easy way to draw chronological charts from tables, aiming to include an intuitive environment for anyone new to R. Includes’ ggplot2’ geoms and theme for chronological charts. There are two vignettes: ChronochRt and ChronochRt_examples. Additionally, I want to mention: plotly: Create Interactive Web Graphics via ‘plotly.js’). Create interactive web graphics from ‘ggplot2’ graphs and/or a custom interface to the (MIT-licensed) JavaScript library ‘plotly.js’ inspired by the grammar of graphics. There is also a book on plotly. autoplotly: Automatic Generation of Interactive Visualizations for Statistical Results. Functionalities to automatically generate interactive visualizations for statistical results supported by ggfortify, such as time series, PCA, clustering, and survival analysis, with ‘plotly.js’ https://plotly.com/ and ‘ggplot2’ style. The generated visualizations can also be easily extended using ‘ggplot2’ and ‘plotly’ syntax while staying interactive. See vignette Introduction to autoplotly package. Investigate the above-mentioned packages Search for more graphic packages via the term ‘graph,’ ‘chart,’ or similar. It seems that getting an overview of visualization packages would be an essential but time-consuming task. I wonder why there is no CRAN task view specialized in visualization. 1.4.3 Data und models plot Data plots: What do the data look like? Are there unusual features? What kinds of summaries would be helpful? Model plots: What does the model “look” like? (plot predicted values); How does the model change when its parameters change? (plot competing models); How does the model change when the data is changed? (e.g., influence plots). Data + Model plots: How well does a model fit the data? Does a model fit uniformly good or bad, or just good/bad in some regions? How can a model be improved? Model uncertainty: show confidence/prediction intervals or regions. Data support: where is data too “thin” to make a difference in competing models? 1.5 The 80-20-rule 20% of your effort can generate 80% of your desired result in producing a given plot. (Pareto principle or 80-20 rule) 80% of your effort may be required to produce the remaining 20% of a finished graph. In the labs part of the book, I found two interesting links: The top ten worst graphs The R Graph Gallery There are other R visualization gallery sites as well. Prominent to mention is here the shiny gallery. I think it would pay the effort to search for other specialized pages on visualization. 1.6 Visualization of distributions There is a short mathematical resume about the most important distribution followed by a typical visualization. The function expand.grid() creates a data frame with all the possible combinations of interest. The examples choose several successes out of a row of trials. With one exception, the graphs are not done with {ggplot2} but with lattice. But it should be easy to convert them with ggplot(). I did not do it because I just wanted to get an overview of the argumentation during the first reading. Essential, however, was the understanding of expand.grid() where I did some experiments. I also checked the replacement of with() with the filter() function from the {dplyr} package. It is easy done but with() returns numeric vector whereas filter() returns a data frame. I believe this difference is an advantage of {dplyr}. with(mtcars, mpg[cyl == 8 &amp; disp &gt; 350]) ## [1] 18.7 14.3 10.4 10.4 14.7 19.2 15.8 dplyr::filter(mtcars, cyl == 8 &amp; disp &gt; 350) |&gt; dplyr::select(mpg) ## mpg ## Hornet Sportabout 18.7 ## Duster 360 14.3 ## Cadillac Fleetwood 10.4 ## Lincoln Continental 10.4 ## Chrysler Imperial 14.7 ## Pontiac Firebird 19.2 ## Ford Pantera L 15.8 References "],["working-with-categorical-data.html", "2 Working with categorical data 2.1 Introduction 2.2 Forms of categorical data 2.3 A complex example", " 2 Working with categorical data 2.1 Introduction Creating and manipulating categorical data sets requires some skills and techniques in R beyond those ordinarily used for quantitative data. This chapter illustrates these for the main formats for categorical data: case form, frequency form, and table form. The chapter uses many base R commands I am not familiar with. But I believe that most of these commands could be translated to the tidy data approach. The praxis will show if I have the skills for these transformations. This chapter will focus on those commands where I lack knowledge of their functionality. For example, there is a focus on tables, matrices, and arrays – data structures I am not comfortable with. But it should be easy to convert them to data frames or tibbles. There is a similar situation with the graphic command plot(), which should be replaced with ggplot(). 2.2 Forms of categorical data The book distinguishes between case, frequency, and table form. I believe that this differentiation is with the tidy approach not essential anymore. Nowadays, more critical is converting data from wide to long with pivot_longer() and vice versa with pivot_wider(). But again: Practical challenges will be the proof of my assessment. 2.2.1 Using tabulizer Instead of entering the data manually or importing it from a file or website, I was experimenting with the {tabulizer} package. ‘Tabula’ is a Java library designed to extract tables from PDF documents computationally. Sometimes there are problems with the employment as {rJava} is required and complicated to install. But I had it already installed and can’t remember any issues. {tabulizerjars} is also necessary. The only purpose of {tabulizerjars} is to distribute releases of the ‘Tabula’ Java library to be used with the {tabulizer} package. Note that {tabulizerjars} itself does not provide any functionality apart from linking to the Java version installed on the system. See the Tabula webpage but first and foremost the rOpenSci web page on tabulizer for more details and documentation of all the features. The primary function is extract_tables() with several parameters, for instance, the page for the table to be extracted. But in my case, providing the page number pulled another table at the beginning of the page. Therefore I had to use the interactive locate_areas() function. The program stops and displays a mini picture of the desired PDF page to select the area with the table. library(tidyverse) library(tabulizer) library(tabulizerjars) f &lt;- normalizePath(&quot;test-data/DDAR.pdf&quot;) tab_area &lt;- locate_areas(file = f, page = 53) saveRDS(tab_area, &quot;test-data/example-tab-area&quot;) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 2.1.1 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tabulizer) library(tabulizerjars) f &lt;- normalizePath(&quot;test-data/DDAR.pdf&quot;) tab_area &lt;- readRDS(&quot;test-data/example-tab-area&quot;) df &lt;- extract_tables(file = f, page = 53, area = tab_area, guess = FALSE, output = &quot;data.frame&quot;) df &lt;- tibble(df[[1]]) colnames(df) &lt;- as.character(df[1, ]) df &lt;- df[-1,] df &lt;- pivot_longer(df, 2:4, names_to = &quot;party&quot;, values_to = &quot;count&quot;) df ## # A tibble: 6 × 3 ## sex party count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 female dem 279 ## 2 female indep 73 ## 3 female rep 225 ## 4 male dem 165 ## 5 male indep 47 ## 6 male rep 191 2.2.2 Generating tables With data in case form or frequency form, you can generate frequency tables from factor variables in data frames using the table() function; for tables of proportions, use the prop.table() function, and for marginal frequencies (summing over some variables) use margin.table(). I have to look up other books using modern approaches to see what functions they use, e.g., ModernDive (Statistical Inference via Data Science – A ModernDive into R and the Tidyverse). I believe the {janitor} packages may be helpful for prop.table() and margin.table() functionalities. 2.2.3 Printing tables For 3-way and larger tables, the functions ftable() (in the {stats} package) and structable() (in {vcd}) provide a convenient and flexible tabular display in a “flat” (2-way) format. 2.2.4 Collapsing tables It sometimes happens that we have a data set with more variables or factors than we want to analyze. The book recommends aggregate() in {stats}, margin.table() resp. marginSums(), apply() (both in {base}), and collapse.table() from the {vcdExtra} package. Again I have to find equivalents. I believe that the modern approach works with {dplyr}, {tidyr}, {purrr}, {tibble} and {forcats} (all provided by the {tidyverse} package suite.) 2.2.5 Converting tables A given contingency table can be represented equivalently in case, frequency, and table forms. However, some R functions were designed for one particular representation. Therefore converting tables from one structure to another is critical. In addition to the already mentioned functions of table(), xtabs() and as.data.frame() the book references the expand.dft() function in {vcdExtra}. For producing a LateX table the {xtable} package is recommended. Am not sure if the packages of the {tidyverse} meta-package solve these conversion problems. But to know more about the requirements, I will print the output from different table commands. I will not use one of the sophisticated table formating and print commands but will apply just the implemented printing method of the referenced table command. I am going to use for this task the UCBAdmissions data from the base {datasets} package. For a shorter later reference, I will copy it to UCB. 2.2.5.1 table (UCB &lt;- UCBAdmissions) # same as: xtabs(Freq ~ ., data = UCB) ## , , Dept = A ## ## Gender ## Admit Male Female ## Admitted 512 89 ## Rejected 313 19 ## ## , , Dept = B ## ## Gender ## Admit Male Female ## Admitted 353 17 ## Rejected 207 8 ## ## , , Dept = C ## ## Gender ## Admit Male Female ## Admitted 120 202 ## Rejected 205 391 ## ## , , Dept = D ## ## Gender ## Admit Male Female ## Admitted 138 131 ## Rejected 279 244 ## ## , , Dept = E ## ## Gender ## Admit Male Female ## Admitted 53 94 ## Rejected 138 299 ## ## , , Dept = F ## ## Gender ## Admit Male Female ## Admitted 22 24 ## Rejected 351 317 2.2.5.2 ftable() ftable() creates ‘flat’ contingency tables. They contain the counts of each combination of the levels of the variables (factors) involved. This information is then re-arranged as a matrix whose rows and columns correspond to unique combinations of the levels of the row and column variables (as specified by row.vars and col.vars, respectively). These combinations are created by looping over the variables in reverse order so that levels of the left-most variable vary the slowest. Displaying a contingency table in this flat matrix form is often preferable to showing it as a higher-dimensional array. ftable() returns an object of class “ftable,” which is a matrix with counts of each combination of the levels of variables with information on the names and levels of the (row and columns) variables stored as attributes “row.vars” and “col.vars.” ftable(UCB, row.vars = 1:2) ## Dept A B C D E F ## Admit Gender ## Admitted Male 512 353 120 138 53 22 ## Female 89 17 202 131 94 24 ## Rejected Male 313 207 205 279 138 351 ## Female 19 8 391 244 299 317 2.2.5.3 ftable() with formula The left and right-hand sides of the formula specify the column and row variables, respectively, of the flat contingency table to be created. Only the + operator is allowed for combining the variables. A . may be used once in the formula to indicate the inclusion of all the remaining variables. If data is an object of class “table” or an array with more than 2 dimensions, it is taken as a contingency table, and hence all entries should be nonnegative. Otherwise, if it is not a flat contingency table (i.e., an object of class “ftable”), it should be a data frame or matrix, list, or environment containing the variables to be cross-tabulated. The result is a flat contingency table that contains the counts of each combination of the levels of the variables, collapsed into a matrix for suitably displaying the counts. ftable(Admit + Gender ~ Dept, data = UCB) ## Admit Admitted Rejected ## Gender Male Female Male Female ## Dept ## A 512 89 313 19 ## B 353 17 207 8 ## C 120 202 205 391 ## D 138 131 279 244 ## E 53 94 138 299 ## F 22 24 351 317 2.2.5.4 xtabs() The xtabs() function allows you to create cross-tabulations of data using formula-style input. This typically works with case- or frequency-form data supplied in a data frame or a matrix. The result is a contingency table in array format, whose dimensions are determined by the formula’s terms on the right side. xtabs(Freq ~ Gender + Admit, data = UCB) ## Admit ## Gender Admitted Rejected ## Male 1198 1493 ## Female 557 1278 2.2.5.5 structable() structable() produces a ‘flat’ representation of a high-dimensional contingency table constructed by recursive splits (similar to the construction of mosaic displays). The result is a textual representation of mosaic displays and thus ‘flat’ contingency tables. The formula interface is similar to ftable() but also accepts the mosaic-like formula interface (empty left-hand side). The function results in an object of class “structable,” inheriting from class “ftable,” with the splitting information (“split_vertical”) as an additional attribute. vcd::structable(UCB) ## Gender Male Female ## Admit Dept ## Admitted A 512 89 ## B 353 17 ## C 120 202 ## D 138 131 ## E 53 94 ## F 22 24 ## Rejected A 313 19 ## B 207 8 ## C 205 391 ## D 279 244 ## E 138 299 ## F 351 317 2.2.5.6 tibble() The text of this section is derived from the help files and vignette of the {tibble} packages. tibble() is a nice way to create data frames. A tibble is a data frame with class tbl_df, a subclass of data.frame, designed to have different default behavior. Tibble is the central data structure for the set of packages known as the {tidyverse}. To complement tibble(), the {tibble} package provides as_tibble() to coerce an existing object, such as a data frame or matrix, into tibbles . A tibble encapsulates best practices for data frames: It never changes an input’s type (i.e., no more stringsAsFactors = FALSE!). This makes it easier to use with list columns. List-columns are often created by tidyr::nest(), but they can be useful to create by hand. It never adjusts the names of variables. It evaluates its arguments lazily and sequentially. It never uses row.names(). The whole point of tidy data is to store variables in a consistent way. So it never stores a variable as a special attribute. It only recycles vectors of length 1. This is because recycling vectors of greater lengths are a frequent source of bugs. (as_tibble(UCB)) ## # A tibble: 24 × 4 ## Admit Gender Dept n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Admitted Male A 512 ## 2 Rejected Male A 313 ## 3 Admitted Female A 89 ## 4 Rejected Female A 19 ## 5 Admitted Male B 353 ## 6 Rejected Male B 207 ## 7 Admitted Female B 17 ## 8 Rejected Female B 8 ## 9 Admitted Male C 120 ## 10 Rejected Male C 205 ## # … with 14 more rows It seems to me that all the complex discussion about different table formats is now outdated with the {tibble} data structure from the {tidyverse} approach. The same is true with {dplyr} concerning subsetting, filtering, or other transforming activities. This is especially important for manipulating factor levels where the {forcats} packages replace all the different functions for aggregating and collapsing. But to learn &amp; decide what code is necessary, I would need the actual problem. Therefore I just skimmed chapter 2. 2.3 A complex example A good conversion exercise is the following complex example on TV viewing data. I will check if I can provide the necessary R code to fulfill the requirements. I have to inspect it line per line and replace it with working code from the newer tidyverse approach. To see the difference, I will keep the old code and describe the various data-wrangling actions. 2.3.1 Dataset description Are you ready for a more complicated example that puts together various skills developed in this chapter? These skills are reading raw data, creating tables, assigning level names to factors and collapsing levels or variables for use in the analysis. To illustrate these steps, we use the dataset tv.dat, supplied with the initial implementation of mosaic displays. They were derived from an early, compelling example of mosaic displays that illustrated the method with data on a large sample of TV viewers whose behavior had been recorded for the Neilsen ratings. This data set contains sample television audience data from Neilsen Media Research for the week starting November 6, 1995. The data file, tv.dat, is stored in frequency form with 825 rows and 5 columns. There is no header line in the file, so when we use read.table() below, the variables will be named V1 – V5. This data represents a 4-way table of size \\(5 × 11 × 5 × 3 = 825\\) where the table variables are V1 – V4, and the cell frequency is read as V5. The table variables are: V1 — values \\(1:5\\) correspond to the days Monday–Friday; V2 — values \\(1:11\\) correspond to the quarter-hour times \\(8:00\\) pm through \\(10:30\\) pm; V3 — values \\(1:5\\) correspond to ABC, CBS, NBC, Fox, and non-network choices; V4 — values \\(1:3\\) correspond to transition states: turn the television Off, Switch channels, or Persist in viewing the current channel. 2.3.2 Package dataset There is a TV dataset in the {vcdExtra} package. To load it would be the easiest way to get the data. In that case, you would not have to worry about data transformation because the dataset is already in the desired form. library(vcdExtra) ## Loading required package: vcd ## Loading required package: grid ## Loading required package: gnm ## ## Attaching package: &#39;vcdExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## summarise data(TV) # the most straightforward way, does not need data wrangling TV ## , , Network = ABC ## ## Time ## Day 8:00 8:15 8:30 8:45 9:00 9:15 9:30 9:45 10:00 10:15 10:30 ## Monday 146 151 156 83 325 350 386 340 352 280 278 ## Tuesday 244 181 231 205 385 283 345 192 329 351 364 ## Wednesday 233 161 194 156 339 264 279 140 237 228 203 ## Thursday 174 183 197 181 187 198 211 86 110 122 117 ## Friday 294 281 305 239 278 246 245 138 246 232 233 ## ## , , Network = CBS ## ## Time ## Day 8:00 8:15 8:30 8:45 9:00 9:15 9:30 9:45 10:00 10:15 10:30 ## Monday 337 293 304 233 311 251 241 164 252 265 272 ## Tuesday 173 180 184 109 218 235 256 250 274 263 261 ## Wednesday 158 126 207 59 98 103 122 86 109 105 110 ## Thursday 196 185 195 104 106 116 116 47 102 84 84 ## Friday 130 144 154 81 129 153 136 126 138 136 152 ## ## , , Network = NBC ## ## Time ## Day 8:00 8:15 8:30 8:45 9:00 9:15 9:30 9:45 10:00 10:15 10:30 ## Monday 263 219 236 140 226 235 239 246 279 263 283 ## Tuesday 315 254 280 241 370 214 195 111 188 190 210 ## Wednesday 134 146 166 66 194 230 264 143 274 289 306 ## Thursday 515 463 472 477 590 473 446 349 649 705 747 ## Friday 195 220 248 160 172 164 169 85 183 198 204 str(TV) ## int [1:5, 1:11, 1:3] 146 244 233 174 294 151 181 161 183 281 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ Day : chr [1:5] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; ... ## ..$ Time : chr [1:11] &quot;8:00&quot; &quot;8:15&quot; &quot;8:30&quot; &quot;8:45&quot; ... ## ..$ Network: chr [1:3] &quot;ABC&quot; &quot;CBS&quot; &quot;NBC&quot; ‘TV’ data set comprises a 5 x 11 x 3 contingency table. But this is not the original dataset described under the subsection 2.3.2. “The original data, tv.dat, contains two additional networks:”Fox” and “Other,” with small frequencies. These levels were removed in the current version. There is also a fourth factor, transition State transition (turn the television Off, Switch channels, or Persist in viewing the current channel). The TV data here includes only the Persist observations.” (From the TV {vcdExtra} help file.) We, therefore, will go the hard way and import the tv.dat file as mentioned in the book. But viewing the above dataset gives you a good impression of how the data should look at the end of the data wrangling process. The book version containsbv- in the chunk- and variable names. In contrast to my own version, which has my initials pb- in their designations. 2.3.3 bv-import 1. Step: The provided R code in the ch02.R file does not work. The book referenced the file tv.dat to the doc/extdata directory of {vcdExtra}. On my (macOS) installation tv.dat is indeed inside extdata, but extdata is not a subdirectory of doc. You can also use the RStudio interactive menu: “File -&gt; Import Dataset -&gt; From text (base) ….” 2. Step: In the next step we use xtabs() to do the cross-tabulation, using \\(V5\\) as the frequency variable. xtabs() uses a formula interface as demonstrated in section 2.2.5.4. 3. Step: The third step attaches names to the factors. There is no assignment necessary, but the list has to be ordered. tv_bv &lt;- read.table(system.file(&quot;extdata&quot;, &quot;tv.dat&quot;, # without &quot;doc&quot; directory package = &quot;vcdExtra&quot;)) tv_bv &lt;- xtabs(V5 ~ ., data = tv_bv) dimnames(tv_bv) &lt;- list( Day = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;), Time = c( &quot;8:00&quot;, &quot;8:15&quot;, &quot;8:30&quot;, &quot;8:45&quot;, &quot;9:00&quot;, &quot;9:15&quot;, &quot;9:30&quot;, &quot;9:45&quot;, &quot;10:00&quot;, &quot;10:15&quot;, &quot;10:30&quot; ), Network = c(&quot;ABC&quot;, &quot;CBS&quot;, &quot;NBC&quot;, &quot;Fox&quot;, &quot;Other&quot;), State = c(&quot;Off&quot;, &quot;Switch&quot;, &quot;Persist&quot;) ) tv_bv &lt;- as.data.frame(tv_bv, dim = c(5, 11, 5, 3)) str(tv_bv) ## &#39;data.frame&#39;: 825 obs. of 5 variables: ## $ Day : Factor w/ 5 levels &quot;Mon&quot;,&quot;Tue&quot;,&quot;Wed&quot;,..: 1 2 3 4 5 1 2 3 4 5 ... ## $ Time : Factor w/ 11 levels &quot;8:00&quot;,&quot;8:15&quot;,..: 1 1 1 1 1 2 2 2 2 2 ... ## $ Network: Factor w/ 5 levels &quot;ABC&quot;,&quot;CBS&quot;,&quot;NBC&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ State : Factor w/ 3 levels &quot;Off&quot;,&quot;Switch&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Freq : int 6 18 6 2 11 6 29 25 17 29 ... head(tv_bv, 5) ## Day Time Network State Freq ## 1 Mon 8:00 ABC Off 6 ## 2 Tue 8:00 ABC Off 18 ## 3 Wed 8:00 ABC Off 6 ## 4 Thu 8:00 ABC Off 2 ## 5 Fri 8:00 ABC Off 11 2.3.4 pb-import 1. Step: At first I tried to use the read_table() function from the {readr} package to import the data into a tibble. But this didn’t work, as I always got an additional column of logical type full with NA's, and the first row was not imported. After I changed to the free width formatread_fwf()` of the same package, it worked. In the same step, I named the columns and defined the data type of the variables. The import result is a tibble, a modern take on data frames. It encapsulates best practices for data frames and drops the features that used to be convenient but are now frustrating (i.e., converting character vectors to factors). I also used its superior printing method by using glimpse() instead of str(). Watch the difference between bv and pb version: The function read.table() (with a period) in bv is called from the {base} package, whereas read_table() (with an underscore) in pb is part of {readr} and has to be loaded as a library. 2.Step: In the next step I recoded all factor labels with fct_recode() from the {forcats} package. I overwrote their old values with mutate() from the {dplyr} package. I linked all commands of the sequence with the pipe operator %&gt;% using a different appearance |&gt; with a particular font. All the mentioned packages — including the pipe operator — are part of the {tidyverse} meta-package, which has to be loaded. library(tidyverse) tv_pb &lt;- read_fwf(system.file(&quot;extdata&quot;, &quot;tv.dat&quot;, # without the &quot;doc&quot; directory package = &quot;vcdExtra&quot;), col_types = &#39;ffffi&#39;, fwf_widths(c(2, 2, 2, 2, 2), col_names = c(&quot;Day&quot;, &quot;Time&quot;, &quot;Network&quot;, &quot;State&quot;, &quot;Freq&quot;) )) |&gt; mutate(Day = fct_recode( Day, Mon = &quot;1&quot;, Tue = &quot;2&quot;, Wed = &quot;3&quot;, Thu = &quot;4&quot;, Fri = &quot;5&quot; )) |&gt; mutate( Time = fct_recode( Time, &quot;8:00&quot; = &quot;1&quot;, &quot;8:15&quot; = &quot;2&quot;, &quot;8:30&quot; = &quot;3&quot;, &quot;8:45&quot; = &quot;4&quot;, &quot;9:00&quot; = &quot;5&quot;, &quot;9:15&quot; = &quot;6&quot;, &quot;9:30&quot; = &quot;7&quot;, &quot;9:45&quot; = &quot;8&quot;, &quot;10:00&quot; = &quot;9&quot;, &quot;10:15&quot; = &quot;10&quot;, &quot;10:30&quot; = &quot;11&quot; ) ) |&gt; mutate(Network = fct_recode( Network, &quot;ABC&quot; = &quot;1&quot;, &quot;CBS&quot; = &quot;2&quot;, &quot;NBC&quot; = &quot;3&quot;, &quot;Fox&quot; = &quot;4&quot;, &quot;Other&quot; = &quot;5&quot; )) |&gt; mutate(State = fct_recode( State, &quot;Off&quot; = &quot;1&quot;, &quot;Switch&quot; = &quot;2&quot;, &quot;Persist&quot; = &quot;3&quot; )) glimpse(tv_pb) ## Rows: 825 ## Columns: 5 ## $ Day &lt;fct&gt; Mon, Tue, Wed, Thu, Fri, Mon, Tue, Wed, Thu, Fri, Mon, Tue, We… ## $ Time &lt;fct&gt; 8:00, 8:00, 8:00, 8:00, 8:00, 8:15, 8:15, 8:15, 8:15, 8:15, 8:… ## $ Network &lt;fct&gt; ABC, ABC, ABC, ABC, ABC, ABC, ABC, ABC, ABC, ABC, ABC, ABC, AB… ## $ State &lt;fct&gt; Off, Off, Off, Off, Off, Off, Off, Off, Off, Off, Off, Off, Of… ## $ Freq &lt;int&gt; 6, 18, 6, 2, 11, 6, 29, 25, 17, 29, 10, 10, 12, 8, 7, 20, 24, … head(tv_pb, 5) ## # A tibble: 5 × 5 ## Day Time Network State Freq ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Mon 8:00 ABC Off 6 ## 2 Tue 8:00 ABC Off 18 ## 3 Wed 8:00 ABC Off 6 ## 4 Thu 8:00 ABC Off 2 ## 5 Fri 8:00 ABC Off 11 2.3.5 Summary After some troubles with the import command (using read_fwf() instead of read_table()), I had no difficulties converting the code to the newer tidyverse approach. The book version uses for re-coding the implicit order of the factor levels. The bv-code is, therefore, shorter than my pb-conversion. On the other hand, the tidyverse version is perhaps more neatly arranged and better understandable. But I admit that there is no advantage over the book version in this example. The goal was to demonstrate if I could manage to convert the book’s code into a version where I do not need to use the special table commands like ftable(), xtable(), structable(). At least with the example of xtable(), I have succeeded. "],["still-to-do.html", "3 Still to do", " 3 Still to do "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
