[["index.html", "Discrete Data Analysis with R 1 Preface", " Discrete Data Analysis with R Peter Baumgartner 2022-02-08 1 Preface Friendly, M., &amp; Meyer, D. (2015). Discrete Data Analysis with {R}: Visualization and Modeling Techniques for Categorical and Count Data (1st ed.). Chapman and Hall/CRC. My notes on (Friendly and Meyer 2015) are part excerpt and part personal notes (comments). Most of the text are quotes taken directly from the book, which I have not marked especially. Sometimes I commented or put forward my own thought. In those cases I have formatted the text as a quotation. This is an example of a paragraph marked as a quotation. These type of formatted text passages present — contrary to the usual formatting habits — my own thoughts, whereas excerpts or quotes from the textbook is not marked specially. R scripts and data files are taken from the book website. The book is supported directly by the R packages {vcd} and {vcdExtra}, along with numerous other R packages. There is a list of packages used a list of the data sets used in the book, by package and a list of files with R code per chapter or as a zip file. References "],["introduction.html", "2 Introduction 2.1 Overview 2.2 What is categorical data? 2.3 Data analysis strategies 2.4 Graphical methods 2.5 The 80-20-rule 2.6 Visualization of distributions", " 2 Introduction Friendly, M., &amp; Meyer, D. (2015). Discrete Data Analysis with R: Visualization and Modeling Techniques for Categorical and Count Data (1st ed.). Chapman and Hall/CRC. Short name (abbreviation): DDAR 2.1 Overview “Discrete Data Analysis with R” (Friendly and Meyer 2015) has at its strength the interplay between analysis and visualization strategies for categorical data. The authors drive the conviction that data analysis and statistical graphics should go hand-in-hand in the process of understanding and communicating statistical data. The book features the well-known {vcd} package (Visualizing Categorical Data), and the books support package {vcdExtra} (‘vcd’ Extensions and Additions), which provides additional data sets and methods. Both packages come with large tutorial vignettes explaining the usage. See for instance: Working with categorical data with R and the vcd and vcdExtra packages: 37 pages The Strucplot Framework: Visualizing Multi-way Contingency Tables with vcd: 48 pages The book divides statistical methods for data analysis into two different categories: Randomized-based methods: Pearson chi-squared \\(X^2\\) test, Fisher’s exact test, and Cochran–Mantel–Haenszel tests (Chapter 3-6) and Model-based methods: Logistic regression, loglinear, and generalized linear models (Chapter 7-9). 2.2 What is categorical data? Categorical variables differ in the number of categories: binary variables (or dichotomous variables) such as Gender: “Male,” “Female” multilevel variables, variables with more than two categories (called polytomous variables), e.g., Fielding position (in baseball), with categories “Pitcher,” “Catcher,” “1st base,” “2nd base,” …, “Left field.” their relation: ordered categories: e.g., Treatment outcome: “No improvement,” “Some improvement,” “Marked improvement.” nominal categories: e.g., Marital Status: “Never married,” “Married,” “Separated,” “Divorced,” “Widowed.” Often, we can rank some variable under a specific point of view. For instance, we could arrange the variable side effect in a pharmacological study according to the seriousness of the side effects or party preference according to the number of party voters in the last election. But these kinds of classifications are somewhat arbitrary. An ordinal variable should be defined as one whose categories are unambiguously ordered along a single underlying dimension. Examples are levels of education, for instance the nine levels of the International Standard Classification of Education (ISCED): early childhood education (level 0); primary education (level 1); lower secondary education (level 2); upper secondary education (level 3); postsecondary non-tertiary education (level 4); short-cycle tertiary education (level 5); bachelor’s or equivalent level (level 6); master’s or equivalent level (level 7); doctor or equivalent level (level 8) or the Likert scale in survey questions (“strongly agree,” “agree,” “neutral,” “disagree,” “strongly disagree.”). 2.2.1 Case vs. frequency form In many circumstances, data is recorded on each individual or experimental unit. Data in this form is called case data, or data in case form. Data in frequency form, by contrast, has already been tabulated by counting over the categories of the table variables. If observations are recorded from operationally independent experimental units or individuals, typically a sample from some population, this tabulated data may be called frequency data. However, suppose several events or variables are observed for the same units or individuals. Those events are not operationally independent, and it is helpful to use the term count data in this situation. These terms are by no means standard, but the distinction is often essential, particularly in statistical models for categorical data. For example, in a tabulation of the number of male children within families, the number of male children in a given family would be a count variable, taking values 0, 1, 2, …. The number of independent families with a given number of male children is a frequency variable. Count data also arise when we tabulate a sequence of events over time or under different circumstances in a number of individuals. 2.2.2 Univariate, bivariate, and multivariate data Another distinction concerns the number of variables: one, two, or (potentially) many shown in a data set or table or used in some analysis. Most statistical models distinguish between response variables (or dependent or criterion variables) and explanatory variables (or independent or predictor variables). 2.2.3 Explanatory vs. response variables Response Variable (also dependent or criterion variable) is either the treatment outcome (primarily quantitative), e.g., mean -&gt; normal distribution or variable of primary interest (e.g., educational achievement level predicted from socioeconomic status) In the classical models, the response variable (“treatment outcome,” for example) must be considered quantitative. The model attempts to describe how the mean of the distribution of responses changes with the values or levels of the explanatory variables, such as age or gender. However, when the response variable is categorical, the standard linear models do not apply because they assume a normal (Gaussian) distribution for the model residuals. Explanatory Variable (independent or predictor variable) experimentally manipulated, e.g., the treatment group each person is assigned to or not manipulated (e.g., socioeconomic status), but used to predict another variable Hence, a categorical response variable generally requires analysis using methods for categorical data, but categorical explanatory variables may be readily handled by either method. 2.3 Data analysis strategies 2.3.1 Hypothesis testing approaches Many studies raise questions concerning hypotheses about the association between variables, a more general idea than that of correlation (linear association) for quantitative variables. If a non-zero association exists, we may wish to characterize the strength of the association numerically and understand the pattern or nature of the association. Examples for Questions: “Is there evidence of gender bias in admission to graduate school?” Another way to frame this: “Are males more likely to be admitted?” Questions involving tests of such hypotheses are answered most easily using a large variety of specific statistical tests, often based on randomization arguments. These include the familiar Pearson chi-squared test for two-way tables, the Cochran–Mantel–Haenszel test statistics, Fisher’s exact test, and a wide range of measures of strength of association. The hypothesis testing approach is illustrated in Chapters 4–6 of DDAR, though the emphasis is on graphical methods that help us understand the nature of the association between variables. Illustration of hypothesis testing The approach is demonstrated with the HairEyeColor from the base R {datasets} package. As a graphical tool, it uses mosaic display mosaicplot() from the base R {graphics} package. (see Chapter 5 of DDAR) and correspondence analysis plot ca() from the {ca} package (see Chapter 6 of DDAR) For the test calculation, it uses chisq.test() from the base R {stats} package and assocstats() from the {vcd} package. I do not know how to apply these tools. I will postpone my reading until chapters 5 and 6. 2.3.2 Model building approaches Model-based methods provide tests of equivalent hypotheses about associations but offer additional advantages (at the cost of additional assumptions) not provided by the simpler hypotheses-testing approaches. Among these advantages, model-based methods provide estimates, standard errors, and confidence intervals for parameters and obtain predicted (fitted/expected) values with associated precision measures. Illustration of model building The book presents for demonstration logistic regression of dichotomous response variables. It illustrates the approach with two important real-world examples: Space shuttle disaster: dataset SpaceShuttle from {vcd}. Survivals of the Donner party (1846/47 Sierra Nevada): dataset Donner from {vcdExtra}. Again I do not understand fully how the model building is done. I am looking forward to reading chapter 7 of DDAR. 2.4 Graphical methods 2.4.1 Visualization recommendations This book section presents several advantages of data visualization. Note that data analysis and graphing are iterative processes: Visualization = Graphing + Fitting + Graphing + Fitting + Graphing … Presentation vs. Exploration: Different communication purposes require different graphs. There is, for instance, a big difference between graphs for presentation and for data exploring. The book’s text links to Statistical Graphs and More a blog on visualization by Martin Theus. There are – for instance – on the blog side column several links to interesting books by the blog author and other scientists I will mention here (but not include them in the bibliography of this book) for my further involvement with the visualization subject: Some books on visualization topics - Cook, D., &amp; Swayne, D. F. (2007). Interactive and Dynamic Graphics for Data Analysis. - Theus, M., &amp; Urbanek, S. (2019). Interactive graphics for data analysis: Principles and examples. - Unwin, A., Theus, M., &amp; Hofmann, H. (2016). Graphics of Large Datasets: Visualizing a million. Springer-Verlag New York. Different graphical methods: Frequencies of categorical variables are often best represented graphically using areas rather than as positions along a scale. Three examples are just illustrated with pictures. There is no R code because the images only serve as a reference for more detailed explanations in later chapters. I will link these three examples to more detailed web pages: mosaic plot (a modified bar chart), fourfold display, and agreement chart. Details on these visualization methods are provided in later chapters. Effect ordering: Alphabetically sorted labels for ordered categories result in most of the time in nonsensical display. Therefore sort the data by the effects to be seen to facilitate comparison. This so-called “effect-order sorting” in combination with a colored representation is also possible with the data fields of the table itself. Interactive and dynamic graphs: Graphics displayed in print form are necessarily static and fixed when they are designed and rendered as an image. Yet, recent developments in software, web technology, and media alternative to print have created the possibility to extend graphics in far more helpful and exciting ways for both presentation and analysis purposes. Interactive graphics allow the viewer to directly manipulate the statistical and visual components of the graphical display. These range from graphical controls (sliders, selection boxes, and other widgets) to control details of analysis (e.g., a smoothing parameter) or graph (colors and other graphic details), to higher-level interaction, including zooming in or out, drilling down to a data subset, linking multiple displays, selecting terms in a model, and so forth. The substantial effect is that the analysis and/or display is immediately re-computed and updated visually. In addition, dynamic graphics use animation to show a series of views, like frames in a movie. Adding time as an additional dimension allows far more possibilities, for example showing a rotating view of a 3D graph or showing smooth transitions or interpolations from one view to another. There are now many packages in R providing interactive and dynamic plots. I will only mention them here, planning to look into the details of these packages. Note that the references are from 2015, meaning that there are now even more packages providing interactive graphics and animated plots. On the other hand, some of the mentioned packages may be outdated. 2.4.2 Recommended Packages Packages for interactive and dynamic plots rggobi: A command-line interface to ‘GGobi,’ an interactive and dynamic graphics package. ‘Rggobi’ complements the graphical user interface of ‘GGobi,’ providing a way to fluidly transition between analysis and exploration, as well as automating common tasks. But the package was removed from the CRAN repository. Older version are in the archive. – GGobi is not an R package but an open-source visualization program for exploring high-dimensional data. It provides highly dynamic and interactive graphics such as tours and familiar graphics such as the scatterplot, bar chart, and parallel coordinates plots. Plots are interactive and linked with brushing and identification. GGobi is fully documented in the GGobi book Interactive and Dynamic Graphics for Data Analysis. There are many references on using GGobi with R, including code examples. iPlots: interactive graphics for R, see the website for detailed information and tutorials. (Last change: June 17, 2018.) rCharts: rCharts is an R package to create, customize and publish interactive javascript visualizations from R using a familiar lattice style plotting interface. See also the not related website R Charts with the especially interesting ggplot2 section. ggogleVis: R interface to Google’s chart tools, allowing users to create interactive charts based on data frames. Charts are displayed locally via the R HTTP help server. A modern browser with an Internet connection is required. The data remains local and is not uploaded to Google. Several vignettes demonstrate the application. ggvis: Interactive Grammar of Graphics. An implementation of an interactive grammar of graphics, taking the best parts of ‘ggplot2,’ combining them with the reactive framework of ‘shiny,’ and drawing web graphics using ‘vega.’ See website at https://ggvis.rstudio.com/. animation: A gallery of animations in statistics and utilities to create animations. See website https://yihui.org/animation/. manipulate: Interactive Plots for RStudio. Interactive plotting functions for use within RStudio. The manipulate function accepts a plotting expression and a set of controls (e.g. slider, picker, checkbox, or button) used to dynamically change values within the expression. The expression is automatically re-executed when a value is changed using its corresponding control, and the plot is redrawn. During searching the link of the above packages I came across two other visualization packages. I will list the for later detailed consulting: apexcharter: Create Interactive Chart with the JavaScript ‘ApexCharts’ Library. Provides an ‘htmlwidgets’ interface to ‘apexcharts.js.’ ‘Apexcharts’ is a modern JavaScript charting library to build interactive charts and visualizations with simple API. See ‘Apexcharts’ examples and documentation at https://apexcharts.com/. chronochrt: Creating Chronological Charts with R. Easy way to draw chronological charts from tables, aiming to include an intuitive environment for anyone new to R. Includes’ ggplot2’ geoms and theme for chronological charts. There are two vignettes: ChronochRt and ChronochRt_examples. Additionally, I want to mention: plotly: Create Interactive Web Graphics via ‘plotly.js’). Create interactive web graphics from ‘ggplot2’ graphs and/or a custom interface to the (MIT-licensed) JavaScript library ‘plotly.js’ inspired by the grammar of graphics. There is also a book on plotly. autoplotly: Automatic Generation of Interactive Visualizations for Statistical Results. Functionalities to automatically generate interactive visualizations for statistical results supported by ggfortify, such as time series, PCA, clustering, and survival analysis, with ‘plotly.js’ https://plotly.com/ and ‘ggplot2’ style. The generated visualizations can also be easily extended using ‘ggplot2’ and ‘plotly’ syntax while staying interactive. See vignette Introduction to autoplotly package. Investigate the above-mentioned packages Search for more graphic packages via the term ‘graph,’ ‘chart,’ or similar. It seems that getting an overview of visualization packages would be an essential but time-consuming task. I wonder why there is no CRAN task view specialized in visualization. 2.4.3 Data und models plot Data plots: What do the data look like? Are there unusual features? What kinds of summaries would be helpful? Model plots: What does the model “look” like? (plot predicted values); How does the model change when its parameters change? (plot competing models); How does the model change when the data is changed? (e.g., influence plots). Data + Model plots: How well does a model fit the data? Does a model fit uniformly good or bad, or just good/bad in some regions? How can a model be improved? Model uncertainty: show confidence/prediction intervals or regions. Data support: where is data too “thin” to make a difference in competing models? 2.5 The 80-20-rule 20% of your effort can generate 80% of your desired result in producing a given plot. (Pareto principle or 80-20 rule) 80% of your effort may be required to produce the remaining 20% of a finished graph. In the labs part of the book, I found two interesting links: The top ten worst graphs The R Graph Gallery There are other R visualization gallery sites as well. Prominent to mention is here the shiny gallery. I think it would pay the effort to search for other specialized pages on visualization. 2.6 Visualization of distributions There is a short mathematical resume about the most important distribution followed by a typical visualization. for this the function expand.grid() creates a data.frame with all the possible combinations of interest. The examples choose several successes out of row of trials. With one exception the graphs are not done with {ggplot2} but with lattice. But it should be easy to convert them with ggplot(). I did not do it, because during the first reading I just want to get an overview about the argumentation. Important however was the understanding of expand.grid() where I did some experiments. I added the description and typical applications of the different distributions to my diigo outliner on categorical data analysis. I also checked the replacement of with() with the filter() function from the {dplyr} package. It is easy done but with() returns numeric vector whereas filter() returns a data frame. I believe this difference is an advantage of {dplyr}. with(mtcars, mpg[cyl == 8 &amp; disp &gt; 350]) ## [1] 18.7 14.3 10.4 10.4 14.7 19.2 15.8 dplyr::filter(mtcars, cyl == 8 &amp; disp &gt; 350) |&gt; dplyr::select(mpg) ## mpg ## Hornet Sportabout 18.7 ## Duster 360 14.3 ## Cadillac Fleetwood 10.4 ## Lincoln Continental 10.4 ## Chrysler Imperial 14.7 ## Pontiac Firebird 19.2 ## Ford Pantera L 15.8 References "],["working-with-categorical-data.html", "3 Working with categorical data 3.1 Introduction 3.2 Forms of categorical data 3.3 A complex example", " 3 Working with categorical data 3.1 Introduction Creating and manipulating categorical data sets requires some skills and techniques in R beyond those ordinarily used for quantitative data. This chapter illustrates these for the main formats for categorical data: case form, frequency form and table form. The chapter uses many base R commands I am not familiar. But I believe that most of these command could be translated to the tidy data approach. The praxis will show, if I have the skills for these transformations. In this chapter I will focus on those command where I lack knowledge of their function either in base R or tidy approach. For example there is a focus on matrices and arrays, data structures I am not comfortable with. But it should be easy to convert them to data frames or tibbles. Similiar with the graphic command plot() which I will replace with ggplot(). 3.2 Forms of categorical data The book distinguishes between case, frequency and table form. I believe that this differentiation is with the tidy approach not essential anymore. More important nowadays is the conversion of data from wide to long with pivot_longer() and vice versa with pivot_wider(). But again: Practical challenges will be the proof of my assessment. 3.2.1 Using tabulizer Instead of entering the data manually or importing from a file or website, I was experimenting with the {tabulizer} package. ‘Tabula’ is a Java library designed to computationally extract tables from PDF documents. Sometimes there are problems with the employment as {rJava} is required and complicated to install. But I had it already installed and cant remember any problems. {tabulizerjars} is also necessary. The only purpose of {tabulizerjars} is to distribute releases of the ‘Tabula’ Java library to be used with the {tabulizer} package. Note that the package itself does not provide any functionality apart from basic linking to Java version installed on the system. See the Tabula webpage but first and foremost the rOpenSci web page on tabulizer for more details and documentation of all the features. The main function is extract_tables() with several parameters, for instance the page for the table to extracted. But in my case to provide the page resulted in the extraction of another table at the beginning of the page. Therefore I had to use the interactive locate_areas() function. The program stops and displays a mini picture of the desired PDF page where one can select the area with the table. library(tidyverse) library(tabulizer) library(tabulizerjars) f &lt;- normalizePath(&quot;test-data/DDAR.pdf&quot;) tab_area &lt;- locate_areas(file = f, page = 53) saveRDS(tab_area, &quot;test-data/example-tab-area&quot;) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 2.1.1 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(tabulizer) library(tabulizerjars) f &lt;- normalizePath(&quot;test-data/DDAR.pdf&quot;) tab_area &lt;- readRDS(&quot;test-data/example-tab-area&quot;) df &lt;- extract_tables(file = f, page = 53, area = tab_area, guess = FALSE, output = &quot;data.frame&quot;) df &lt;- tibble(df[[1]]) colnames(df) &lt;- as.character(df[1, ]) df &lt;- df[-1,] df &lt;- pivot_longer(df, 2:4, names_to = &quot;party&quot;, values_to = &quot;count&quot;) df ## # A tibble: 6 × 3 ## sex party count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 female dem 279 ## 2 female indep 73 ## 3 female rep 225 ## 4 male dem 165 ## 5 male indep 47 ## 6 male rep 191 3.2.2 Generating tables With data in case form or frequency form, you can generate frequency tables from factor variables in data frames using the table() function; for tables of proportions, use the prop.table() function, and for marginal frequencies (summing over some variables) use margin.table(). I have to look up other books using modern approaches to see what functions they use, e.g., ModernDive (Statistical Inference via Data Science – A ModernDive into R and the Tidyverse). I believe the {janitor} packages may be helpful for prop.table() and margin.table() functionalities. 3.2.3 Printing tables For 3-way and larger tables, the functions ftable() (in the {stats} package) and structable() (in {vcd}) provide a convenient and flexible tabular display in a “flat” (2-way) format. 3.2.4 Collapsing tables It sometimes happens that we have a data set with more variables or factors than we want to analyze. The book recommends aggregate() in {stats}, margin.table() resp. marginSums(), apply() both in {base}, and collapse.table()` from the {vcdExtra} package. Again I have to find equivalents. I believe that the modern approach works with {dplyr}, {tidyr}, {purrr}, {tibble} and {forcats} (all provided by the {tidyverse} package suite.) 3.2.5 Converting tables A given contingency table can be represented equivalently in case form, frequency form, and table form. However, some R functions were designed for one particular representation. Therefore converting tables from one form to another, are critical. In addition to the already mentioned functions of table(), xtabs() and as.data.frame() the book references the expand.dft() function in {vcdExtra}. For producing a LateX table the {xtable} package is recommended. Am not sure if the tidyverse() packages solves these conversion problems. But to know more about the requirements I will print the output from different table commands. I am going to use for this task the UCBAdmissions data from the base {datasets} package. For shorter reference I will copy it to UCB. UCBAdmissions:The original structure of the table: (UCB &lt;- UCBAdmissions) ## , , Dept = A ## ## Gender ## Admit Male Female ## Admitted 512 89 ## Rejected 313 19 ## ## , , Dept = B ## ## Gender ## Admit Male Female ## Admitted 353 17 ## Rejected 207 8 ## ## , , Dept = C ## ## Gender ## Admit Male Female ## Admitted 120 202 ## Rejected 205 391 ## ## , , Dept = D ## ## Gender ## Admit Male Female ## Admitted 138 131 ## Rejected 279 244 ## ## , , Dept = E ## ## Gender ## Admit Male Female ## Admitted 53 94 ## Rejected 138 299 ## ## , , Dept = F ## ## Gender ## Admit Male Female ## Admitted 22 24 ## Rejected 351 317 ftable(): ftable(UCB) ## Dept A B C D E F ## Admit Gender ## Admitted Male 512 353 120 138 53 22 ## Female 89 17 202 131 94 24 ## Rejected Male 313 207 205 279 138 351 ## Female 19 8 391 244 299 317 ftable with formula method: # ftable(UCB, row.vars = 1:2) # same result ftable(Admit + Gender ~ Dept, data = UCB) # formula method ## Admit Admitted Rejected ## Gender Male Female Male Female ## Dept ## A 512 89 313 19 ## B 353 17 207 8 ## C 120 202 205 391 ## D 138 131 279 244 ## E 53 94 138 299 ## F 22 24 351 317 as_tibble(): as_tibble(UCB) ## # A tibble: 24 × 4 ## Admit Gender Dept n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Admitted Male A 512 ## 2 Rejected Male A 313 ## 3 Admitted Female A 89 ## 4 Rejected Female A 19 ## 5 Admitted Male B 353 ## 6 Rejected Male B 207 ## 7 Admitted Female B 17 ## 8 Rejected Female B 8 ## 9 Admitted Male C 120 ## 10 Rejected Male C 205 ## # … with 14 more rows structable(): library(vcd) ## Loading required package: grid structable(UCB) ## Gender Male Female ## Admit Dept ## Admitted A 512 89 ## B 353 17 ## C 120 202 ## D 138 131 ## E 53 94 ## F 22 24 ## Rejected A 313 19 ## B 207 8 ## C 205 391 ## D 279 244 ## E 138 299 ## F 351 317 It seems to me that all the complex discussion about different table formats is now outdated with {tibble} from the {tidyverse} approach. The same is true with {dplyr} concerning subsetting, filtering or other transforming activities. This is especially important for manipulating factor levels where the {forcats} packages replaces all the different function for aggregating and collapsing. But to learn &amp; decide what code is necessary I would need the actual problem. Therefore I just skimmed chapter 2. 3.3 A complex example A good conversion exercise is the following complex example on TV viewing data. So I can check if I am able to provide the necessary R code to fulfil the requirements. I have to inspect it line per line and relace it with working code. To see the difference I will keep the old code as comments. (I will do this another time… – I have to separate the different steps with subtitles and comments about my changes.) ### Check with the original example, TV viewing data, p. 58ff. library(vcdExtra) ## reading in the data tv_data &lt;- TV str(tv_data) head(tv_data, 5) ## tv_data &lt;- read.table(&quot;C:/R/data/tv.dat&quot;) ## tv_data &lt;- read.table(file.choose()) ## creating factors within the data frame TV_df &lt;- tv_data colnames(TV_df) &lt;- c(&quot;Day&quot;, &quot;Time&quot;, &quot;Network&quot;, &quot;State&quot;, &quot;Freq&quot;) TV_df &lt;- within(TV_df, { Day &lt;- factor(Day, labels = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;)) Time &lt;- factor(Time) Network &lt;- factor(Network) State &lt;- factor(State) }) ## reshaping the table into a 4-way table TV &lt;- array(tv_data[,5], dim = c(5, 11, 5, 3)) dimnames(TV) &lt;- list(c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;), c(&quot;8:00&quot;, &quot;8:15&quot;, &quot;8:30&quot;, &quot;8:45&quot;, &quot;9:00&quot;, &quot;9:15&quot;, &quot;9:30&quot;, &quot;9:45&quot;, &quot;10:00&quot;, &quot;10:15&quot;, &quot;10:30&quot;), c(&quot;ABC&quot;, &quot;CBS&quot;, &quot;NBC&quot;, &quot;Fox&quot;, &quot;Other&quot;), c(&quot;Off&quot;, &quot;Switch&quot;, &quot;Persist&quot;)) names(dimnames(TV)) &lt;- c(&quot;Day&quot;, &quot;Time&quot;, &quot;Network&quot;, &quot;State&quot;) ## Creating the table using xtabs() TV &lt;- xtabs(V5 ~ ., data = tv_data) dimnames(TV) &lt;- list(Day = c(&quot;Mon&quot;, &quot;Tue&quot;, &quot;Wed&quot;, &quot;Thu&quot;, &quot;Fri&quot;), Time = c(&quot;8:00&quot;, &quot;8:15&quot;, &quot;8:30&quot;, &quot;8:45&quot;, &quot;9:00&quot;, &quot;9:15&quot;, &quot;9:30&quot;, &quot;9:45&quot;, &quot;10:00&quot;, &quot;10:15&quot;, &quot;10:30&quot;), Network = c(&quot;ABC&quot;, &quot;CBS&quot;, &quot;NBC&quot;, &quot;Fox&quot;, &quot;Other&quot;), State = c(&quot;Off&quot;, &quot;Switch&quot;, &quot;Persist&quot;)) ### SECTION ### 2.9.2. Subsetting and collapsing ## subsetting data TV &lt;- TV[,,1:3,] # keep only ABC, CBS, NBC TV &lt;- TV[,,,3] # keep only Persist -- now a 3 way table structable(TV) ## collapsing time labels TV2 &lt;- collapse.table(TV, Time = c(rep(&quot;8:00-8:59&quot;, 4), rep(&quot;9:00-9:59&quot;, 4), rep(&quot;10:00-10:44&quot;, 3))) structable(Day ~ Time + Network, TV2) "],["still-to-do.html", "4 Still to do", " 4 Still to do "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
